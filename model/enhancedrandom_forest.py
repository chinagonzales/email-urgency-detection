# -*- coding: utf-8 -*-
"""EnhancedRandom_Forest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yTc9w0p0BcqaPmTBCbdESSqTKo7MzT_i
"""

!pip install nltk scikit-learn seaborn

from google.colab import files
uploaded = files.upload()

import time
import pandas as pd
import numpy as np
import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.sentiment import SentimentIntensityAnalyzer
from scipy.sparse import hstack
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.utils import resample
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, confusion_matrix
from imblearn.over_sampling import RandomOverSampler
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.metrics import accuracy_score, precision_score, confusion_matrix
from sklearn.model_selection import GridSearchCV, StratifiedKFold
import seaborn as sns
import matplotlib.pyplot as plt


nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('wordnet')
nltk.download('vader_lexicon')

# Load dataset
df = pd.read_csv("all_tickets.csv")

# Data Preprocessing
df.dropna(subset=['body', 'title', 'urgency'], inplace=True)
df['urgency'] = df['urgency'].astype(int)
df['text'] = df['title'] + ' ' + df['body']

feature_extraction_start = time.time()

# Tokenization & Lemmatization
lemmatizer = WordNetLemmatizer()
df['processed_text'] = df['text'].apply(
    lambda text: ' '.join([lemmatizer.lemmatize(word.lower()) for word in word_tokenize(text)])
)

# Feature Engineering
keywords = ['urgent', 'critical', 'asap', 'important', 'immediately', 'immediate', 'as soon as possible',
            'please reply', 'need response', 'emergency', 'high priority', 'time-sensitive', 'priority',
            'top priority', 'urgent matter', 'respond quickly', 'time-critical', 'pressing', 'cruicial',
            'respond promptly', 'without delay']
df['keyword_feature'] = df['processed_text'].apply(lambda text: sum(1 for word in text.split() if word in keywords))

analyzer = SentimentIntensityAnalyzer()
df['sentiment'] = df['processed_text'].apply(lambda x: (analyzer.polarity_scores(x)['compound'] + 1) / 2)

# Train-Test Split
X = df[['processed_text', 'keyword_feature', 'sentiment']]
y = df['urgency']

X_resampled, y_resampled = resample(X, y, stratify=y, n_samples=len(y), random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

ros = RandomOverSampler(random_state=42)
X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)

print("Class Distribution After Resampling:")
print(pd.Series(y_train_resampled).value_counts())

#Ensure X_train and y_train matched after resampling
X_train = pd.DataFrame(X_train_resampled, columns=X_train.columns)
y_train = pd.Series(y_train_resampled)

# TF-IDF Vectorization
vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)
def prepare_combined_features(X_train, X_test, vectorizer):
    tfidf_matrix_train = vectorizer.fit_transform(X_train['processed_text'])
    tfidf_matrix_test = vectorizer.transform(X_test['processed_text'])
    additional_features_train = np.array(X_train[['keyword_feature', 'sentiment']])
    additional_features_test = np.array(X_test[['keyword_feature', 'sentiment']])
    return np.hstack([tfidf_matrix_train.toarray(), additional_features_train]), np.hstack([tfidf_matrix_test.toarray(), additional_features_test])

X_train_combined, X_test_combined = prepare_combined_features(X_train_resampled, X_test, vectorizer)

feature_extraction_time = time.time() - feature_extraction_start

# Define Cross-Validation Strategy
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Initialize Random Forest Model
start_time = time.time()
rf_model = RandomForestClassifier(n_estimators=50, max_depth=15, random_state=42)
training_time = time.time() - start_time

# Define Hyperparameter Grid
param_grid = {
    'n_estimators': [50, 100, 200],  # Number of trees
    'max_depth': [10, 15, 20],       # Tree depth
    'min_samples_split': [2, 5, 10], # Minimum samples to split
    'min_samples_leaf': [1, 2, 4]    # Minimum samples in a leaf
}

# Perform Grid Search with Cross-Validation
grid_search = GridSearchCV(rf_model, param_grid, cv=cv, scoring='accuracy', n_jobs=1, verbose=2)
grid_search.fit(X_train_combined, y_train_resampled)

# Print Best Parameters
print("Best Hyperparameters Found:", grid_search.best_params_)

# Train Final Model with Best Parameters
best_rf_model = grid_search.best_estimator_
best_rf_model.fit(X_train_combined, y_train_resampled)

# Perform Cross-Validation
cv_scores = cross_val_score(rf_model, X_train_combined, y_train_resampled, cv=cv, scoring='accuracy')

# Print Cross-Validation Results
print("Cross-Validation Accuracy Scores:", cv_scores)
print(f"Mean Accuracy: {np.mean(cv_scores):.4f}")
print(f"Standard Deviation: {np.std(cv_scores):.4f}")

# Train Final Model on Full Training Data
rf_model.fit(X_train_combined, y_train_resampled)


# Model Evaluation
def evaluate_model(model, X_test, y_test, title):
    start_time = time.time()
    y_pred = model.predict(X_test)
    evaluation_time = time.time() - start_time
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    cm = confusion_matrix(y_test, y_pred)

    print(f"{title} Results")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Training Time: {feature_extraction_time:.4f} seconds")
    print(f"Prediction Time: {evaluation_time:.4f} seconds")
    print("Confusion Matrix:")
    print(cm)

    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.title(f'{title} - Confusion Matrix')
    plt.show()

# evaluate_model(rf_model, X_test_combined, y_test, "Enhanced Random Forest")
#evaluate_model(rf_model, X_test_combined, y_test, "Enhanced Random Forest with Cross-Validation")
evaluate_model(best_rf_model, X_test_combined, y_test, "Enhanced Random Forest with Grid Search & Cross-Validation")